{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\navin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_BATCH_NORM_DECAY = 0.9\n",
    "_BATCH_NORM_EPSILON = 1e-05\n",
    "_LEAKY_RELU = 0.1\n",
    "_ANCHORS = [(10, 13), (16, 30), (33, 23),\n",
    "            (30, 61), (62, 45), (59, 119),\n",
    "            (116, 90), (156, 198), (373, 326)]\n",
    "_MODEL_SIZE = (416, 416)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Batch norm and fixed padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to define batch_norm function since the model uses batch norms with shared parameters heavily. Also, same as ResNet, Yolo uses convolution with fixed padding, which means that padding is defined only by the size of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(inputs, training, data_format):\n",
    "    \"\"\"Performs a batch normalization using a standard set of parameters.\"\"\"\n",
    "    return tf.layers.batch_normalization(\n",
    "        inputs=inputs, axis=1 if data_format == 'channels_first' else 3,\n",
    "        momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON,\n",
    "        scale=True, training=training)\n",
    "\n",
    "\n",
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "    \"\"\"ResNet implementation of fixed padding.\n",
    "\n",
    "    Pads the input along the spatial dimensions independently of input size.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input to be padded.\n",
    "        kernel_size: The kernel to be used in the conv2d or max_pool2d.\n",
    "        data_format: The input format.\n",
    "    Returns:\n",
    "        A tensor with the same format as the input.\n",
    "    \"\"\"\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],\n",
    "                                        [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end]])\n",
    "    else:\n",
    "        padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end], [0, 0]])\n",
    "    return padded_inputs\n",
    "\n",
    "\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides=1):\n",
    "    \"\"\"Strided 2-D convolution with explicit padding.\"\"\"\n",
    "    if strides > 1:\n",
    "        inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "\n",
    "    return tf.layers.conv2d(\n",
    "        inputs=inputs, filters=filters, kernel_size=kernel_size,\n",
    "        strides=strides, padding=('SAME' if strides == 1 else 'VALID'),\n",
    "        use_bias=False, data_format=data_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Feature extraction: Darknet-53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature extraction, Yolo uses Darknet-53 neural net pre-trained on ImageNet. Same as ResNet, Darknet-53 has shortcut (residual) connections, which help information from earlier layers flow further. We omit the last 3 layers (Avgpool, Connected, and Softmax) since we only need the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def darknet53_residual_block(inputs, filters, training, data_format,\n",
    "                             strides=1):\n",
    "    \"\"\"Creates a residual block for Darknet.\"\"\"\n",
    "    shortcut = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters=filters, kernel_size=1, strides=strides,\n",
    "        data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters=2 * filters, kernel_size=3, strides=strides,\n",
    "        data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs += shortcut\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def darknet53(inputs, training, data_format):\n",
    "    \"\"\"Creates Darknet53 model for feature extraction.\"\"\"\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=32, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=64, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = darknet53_residual_block(inputs, filters=32, training=training,\n",
    "                                      data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=128, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(2):\n",
    "        inputs = darknet53_residual_block(inputs, filters=64,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=256, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=128,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    route1 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=512, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(inputs, filters=256,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    route2 = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=1024, kernel_size=3,\n",
    "                                  strides=2, data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    for _ in range(4):\n",
    "        inputs = darknet53_residual_block(inputs, filters=512,\n",
    "                                          training=training,\n",
    "                                          data_format=data_format)\n",
    "\n",
    "    return route1, route2, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Convolution layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yolo has a large number of convolutional layers. It's useful to group them in blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_convolution_block(inputs, filters, training, data_format):\n",
    "    \"\"\"Creates convolution operations layer used after Darknet.\"\"\"\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=filters, kernel_size=1,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    route = inputs\n",
    "\n",
    "    inputs = conv2d_fixed_padding(inputs, filters=2 * filters, kernel_size=3,\n",
    "                                  data_format=data_format)\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "    return route, inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Detection layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yolo has 3 detection layers, that detect on 3 different scales using respective anchors. For each cell in the feature map, the detection layer predicts n_anchors * (5 + n_classes) values using 1x1 convolution. For each scale we have n_anchors = 3. 5 + n_classes means that respectively to each of 3 anchors we are going to predict 4 coordinates of the box, its confidence score (the probability of containing an object) and class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n",
    "    \"\"\"Creates Yolo final detection layer.\n",
    "\n",
    "    Detects boxes with respect to anchors.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input.\n",
    "        n_classes: Number of labels.\n",
    "        anchors: A list of anchor sizes.\n",
    "        img_size: The input size of the model.\n",
    "        data_format: The input format.\n",
    "\n",
    "    Returns:\n",
    "        Tensor output.\n",
    "    \"\"\"\n",
    "    n_anchors = len(anchors)\n",
    "\n",
    "    inputs = tf.layers.conv2d(inputs, filters=n_anchors * (5 + n_classes),\n",
    "                              kernel_size=1, strides=1, use_bias=True,\n",
    "                              data_format=data_format)\n",
    "\n",
    "    shape = inputs.get_shape().as_list()\n",
    "    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
    "    inputs = tf.reshape(inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1],\n",
    "                                 5 + n_classes])\n",
    "\n",
    "    strides = (img_size[0] // grid_shape[0], img_size[1] // grid_shape[1])\n",
    "\n",
    "    box_centers, box_shapes, confidence, classes = \\\n",
    "        tf.split(inputs, [2, 2, 1, n_classes], axis=-1)\n",
    "\n",
    "    x = tf.range(grid_shape[0], dtype=tf.float32)\n",
    "    y = tf.range(grid_shape[1], dtype=tf.float32)\n",
    "    x_offset, y_offset = tf.meshgrid(x, y)\n",
    "    x_offset = tf.reshape(x_offset, (-1, 1))\n",
    "    y_offset = tf.reshape(y_offset, (-1, 1))\n",
    "    x_y_offset = tf.concat([x_offset, y_offset], axis=-1)\n",
    "    x_y_offset = tf.tile(x_y_offset, [1, n_anchors])\n",
    "    x_y_offset = tf.reshape(x_y_offset, [1, -1, 2])\n",
    "    box_centers = tf.nn.sigmoid(box_centers)\n",
    "    box_centers = (box_centers + x_y_offset) * strides\n",
    "\n",
    "    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1], 1])\n",
    "    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n",
    "\n",
    "    confidence = tf.nn.sigmoid(confidence)\n",
    "\n",
    "    classes = tf.nn.sigmoid(classes)\n",
    "\n",
    "    inputs = tf.concat([box_centers, box_shapes,\n",
    "                        confidence, classes], axis=-1)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Upsample layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to concatenate with shortcut outputs from Darknet-53 before applying detection on a different scale, we are going to upsample the feature map using nearest neighbor interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(inputs, out_shape, data_format):\n",
    "    \"\"\"Upsamples to `out_shape` using nearest neighbor interpolation.\"\"\"\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 2, 3, 1])\n",
    "        new_height = out_shape[3]\n",
    "        new_width = out_shape[2]\n",
    "    else:\n",
    "        new_height = out_shape[2]\n",
    "        new_width = out_shape[1]\n",
    "\n",
    "    inputs = tf.image.resize_nearest_neighbor(inputs, (new_height, new_width))\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Non-max suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is going to produce a lot of boxes, so we need a way to discard the boxes with low confidence scores. Also, to avoid having multiple boxes for one object, we will discard the boxes with high overlap as well using non-max suppression for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_boxes(inputs):\n",
    "    \"\"\"Computes top left and bottom right points of the boxes.\"\"\"\n",
    "    center_x, center_y, width, height, confidence, classes = \\\n",
    "        tf.split(inputs, [1, 1, 1, 1, 1, -1], axis=-1)\n",
    "\n",
    "    top_left_x = center_x - width / 2\n",
    "    top_left_y = center_y - height / 2\n",
    "    bottom_right_x = center_x + width / 2\n",
    "    bottom_right_y = center_y + height / 2\n",
    "\n",
    "    boxes = tf.concat([top_left_x, top_left_y,\n",
    "                       bottom_right_x, bottom_right_y,\n",
    "                       confidence, classes], axis=-1)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,\n",
    "                        confidence_threshold):\n",
    "    \"\"\"Performs non-max suppression separately for each class.\n",
    "\n",
    "    Args:\n",
    "        inputs: Tensor input.\n",
    "        n_classes: Number of classes.\n",
    "        max_output_size: Max number of boxes to be selected for each class.\n",
    "        iou_threshold: Threshold for the IOU.\n",
    "        confidence_threshold: Threshold for the confidence score.\n",
    "    Returns:\n",
    "        A list containing class-to-boxes dictionaries\n",
    "            for each sample in the batch.\n",
    "    \"\"\"\n",
    "    batch = tf.unstack(inputs)\n",
    "    boxes_dicts = []\n",
    "    for boxes in batch:\n",
    "        boxes = tf.boolean_mask(boxes, boxes[:, 4] > confidence_threshold)\n",
    "        classes = tf.argmax(boxes[:, 5:], axis=-1)\n",
    "        classes = tf.expand_dims(tf.to_float(classes), axis=-1)\n",
    "        boxes = tf.concat([boxes[:, :5], classes], axis=-1)\n",
    "\n",
    "        boxes_dict = dict()\n",
    "        for cls in range(n_classes):\n",
    "            mask = tf.equal(boxes[:, 5], cls)\n",
    "            mask_shape = mask.get_shape()\n",
    "            if mask_shape.ndims != 0:\n",
    "                class_boxes = tf.boolean_mask(boxes, mask)\n",
    "                boxes_coords, boxes_conf_scores, _ = tf.split(class_boxes,\n",
    "                                                              [4, 1, -1],\n",
    "                                                              axis=-1)\n",
    "                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n",
    "                indices = tf.image.non_max_suppression(boxes_coords,\n",
    "                                                       boxes_conf_scores,\n",
    "                                                       max_output_size,\n",
    "                                                       iou_threshold)\n",
    "                class_boxes = tf.gather(class_boxes, indices)\n",
    "                boxes_dict[cls] = class_boxes[:, :5]\n",
    "\n",
    "        boxes_dicts.append(boxes_dict)\n",
    "\n",
    "    return boxes_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define the model class using all of the layers described previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Yolo_v3:\n",
    "    \"\"\"Yolo v3 model class.\"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n",
    "                 confidence_threshold, data_format=None):\n",
    "        \"\"\"Creates the model.\n",
    "\n",
    "        Args:\n",
    "            n_classes: Number of class labels.\n",
    "            model_size: The input size of the model.\n",
    "            max_output_size: Max number of boxes to be selected for each class.\n",
    "            iou_threshold: Threshold for the IOU.\n",
    "            confidence_threshold: Threshold for the confidence score.\n",
    "            data_format: The input format.\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "        \"\"\"\n",
    "        if not data_format:\n",
    "            if tf.test.is_built_with_cuda():\n",
    "                data_format = 'channels_first'\n",
    "            else:\n",
    "                data_format = 'channels_last'\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.model_size = model_size\n",
    "        self.max_output_size = max_output_size\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def __call__(self, inputs, training):\n",
    "        \"\"\"Add operations to detect boxes for a batch of input images.\n",
    "\n",
    "        Args:\n",
    "            inputs: A Tensor representing a batch of input images.\n",
    "            training: A boolean, whether to use in training or inference mode.\n",
    "\n",
    "        Returns:\n",
    "            A list containing class-to-boxes dictionaries\n",
    "                for each sample in the batch.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('yolo_v3_model'):\n",
    "            if self.data_format == 'channels_first':\n",
    "                inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "            inputs = inputs / 255\n",
    "\n",
    "            route1, route2, inputs = darknet53(inputs, training=training,\n",
    "                                               data_format=self.data_format)\n",
    "\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=512, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[6:9],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=256, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route2.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            axis = 1 if self.data_format == 'channels_first' else 3\n",
    "            inputs = tf.concat([inputs, route2], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=256, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[3:6],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = conv2d_fixed_padding(route, filters=128, kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "            upsample_size = route1.get_shape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            inputs = tf.concat([inputs, route1], axis=axis)\n",
    "            route, inputs = yolo_convolution_block(\n",
    "                inputs, filters=128, training=training,\n",
    "                data_format=self.data_format)\n",
    "            detect3 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[0:3],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "            inputs = tf.concat([detect1, detect2, detect3], axis=1)\n",
    "\n",
    "            inputs = build_boxes(inputs)\n",
    "\n",
    "            boxes_dicts = non_max_suppression(\n",
    "                inputs, n_classes=self.n_classes,\n",
    "                max_output_size=self.max_output_size,\n",
    "                iou_threshold=self.iou_threshold,\n",
    "                confidence_threshold=self.confidence_threshold)\n",
    "\n",
    "            return boxes_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some utility functions that will help us load images as NumPy arrays, load class names from the official file and draw the predicted boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(img_names, model_size):\n",
    "    \"\"\"Loads images in a 4D array.\n",
    "\n",
    "    Args:\n",
    "        img_names: A list of images names.\n",
    "        model_size: The input size of the model.\n",
    "        data_format: A format for the array returned\n",
    "            ('channels_first' or 'channels_last').\n",
    "\n",
    "    Returns:\n",
    "        A 4D NumPy array.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img = Image.open(img_name)\n",
    "        img = img.resize(size=model_size)\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.concatenate(imgs)\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def load_class_names(file_name):\n",
    "    \"\"\"Returns a list of class names read from `file_name`.\"\"\"\n",
    "    with open(file_name, 'r') as f:\n",
    "        class_names = f.read().splitlines()\n",
    "    return class_names\n",
    "\n",
    "\n",
    "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "    \"\"\"Draws detected boxes.\n",
    "\n",
    "    Args:\n",
    "        img_names: A list of input images names.\n",
    "        boxes_dict: A class-to-boxes dictionary.\n",
    "        class_names: A class names list.\n",
    "        model_size: The input size of the model.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    object_list = list()\n",
    "    repeated_detected_object_list = list()\n",
    "    repeated_detected_object_bounding_boxes = list()\n",
    " \n",
    "    for num, img_name, boxes_dict in zip(range(len(img_names)), img_names,\n",
    "                                         boxes_dicts):\n",
    "        img = Image.open(img_name)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        font = ImageFont.truetype(font='files/futur.ttf',\n",
    "                                  size=(img.size[0] + img.size[1]) // 150)\n",
    "        resize_factor = \\\n",
    "            (img.size[0] / model_size[0], img.size[1] / model_size[1])\n",
    "    ##declaration of empty lists for objects detected and bounding boxes\n",
    "        repeated_detected_object = []\n",
    "        detected_object = []\n",
    "        cordinates_bounding_boxes = []\n",
    "        for cls in range(len(class_names)):\n",
    "            boxes = boxes_dict[cls]\n",
    "            if np.size(boxes) != 0:\n",
    "                color = np.random.permutation([np.random.randint(256), 255, 0])\n",
    "                for box in boxes:\n",
    "                    xy, confidence = box[:4], box[4]\n",
    "                    xy = [xy[i] * resize_factor[i % 2] for i in range(4)]\n",
    "                    x0, y0 = xy[0], xy[1]\n",
    "                    thickness = (img.size[0] + img.size[1]) // 200\n",
    "                    for t in np.linspace(0, 1, thickness//4):\n",
    "                        xy[0], xy[1] = xy[0] + t, xy[1] + t\n",
    "                        xy[2], xy[3] = xy[2] - t, xy[3] - t\n",
    "                        draw.rectangle(xy, outline=tuple(color))\n",
    "                    text = '{}'.format(class_names[cls])\n",
    "\n",
    "                    text_size = draw.textsize(text, font=font)\n",
    "                    draw.rectangle(\n",
    "                        [x0, y0 - text_size[1], x0 + text_size[0], y0],\n",
    "                        fill=tuple(color))\n",
    "                    draw.text((x0, y0 - text_size[1]), text, fill='black',\n",
    "                              font=font)\n",
    "\n",
    "                    ### To find the detected object class\n",
    "                    detected_object.append(class_names[cls])\n",
    "                    cordinates_bounding_boxes.append(boxes_dict[cls])\n",
    "                        \n",
    "        \n",
    "        \n",
    "       # display(img)\n",
    "        repeated_detected_object_list.append(detected_object)\n",
    "        repeated_detected_object_bounding_boxes.append(cordinates_bounding_boxes)\n",
    "        detected_object = list(set(detected_object))\n",
    "        object_list.append(detected_object)\n",
    "    return [object_list, repeated_detected_object_list, repeated_detected_object_bounding_boxes]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(416, 416)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_MODEL_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Converting weights to Tensorflow format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the official weights. We are going to iterate through the file and gradually create tf.assign operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(variables, file_name):\n",
    "    \"\"\"Reshapes and loads official pretrained Yolo weights.\n",
    "\n",
    "    Args:\n",
    "        variables: A list of tf.Variable to be assigned.\n",
    "        file_name: A name of a file containing weights.\n",
    "\n",
    "    Returns:\n",
    "        A list of assign operations.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        # Skip first 5 values containing irrelevant info\n",
    "        np.fromfile(f, dtype=np.int32, count=5)\n",
    "        weights = np.fromfile(f, dtype=np.float32)\n",
    "\n",
    "        assign_ops = []\n",
    "        ptr = 0\n",
    "\n",
    "        # Load weights for Darknet part.\n",
    "        # Each convolution layer has batch normalization.\n",
    "        for i in range(52):\n",
    "            conv_var = variables[5 * i]\n",
    "            gamma, beta, mean, variance = variables[5 * i + 1:5 * i + 5]\n",
    "            batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "            for var in batch_norm_vars:\n",
    "                shape = var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "        # Loading weights for Yolo part.\n",
    "        # 7th, 15th and 23rd convolution layer has biases and no batch norm.\n",
    "        ranges = [range(0, 6), range(6, 13), range(13, 20)]\n",
    "        unnormalized = [6, 13, 20]\n",
    "        for j in range(3):\n",
    "            for i in ranges[j]:\n",
    "                current = 52 * 5 + 5 * i + j * 2\n",
    "                conv_var = variables[current]\n",
    "                gamma, beta, mean, variance =  \\\n",
    "                    variables[current + 1:current + 5]\n",
    "                batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "                for var in batch_norm_vars:\n",
    "                    shape = var.shape.as_list()\n",
    "                    num_params = np.prod(shape)\n",
    "                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                    ptr += num_params\n",
    "                    assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "                shape = conv_var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                    (shape[3], shape[2], shape[0], shape[1]))\n",
    "                var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "            bias = variables[52 * 5 + unnormalized[j] * 5 + j * 2 + 1]\n",
    "            shape = bias.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(bias, var_weights))\n",
    "\n",
    "            conv_var = variables[52 * 5 + unnormalized[j] * 5 + j * 2]\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape(\n",
    "                (shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2, 3, 1, 0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var, var_weights))\n",
    "\n",
    "    return assign_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User input part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### path of the image\n",
    "PATH = 'val2017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PATH + <name of image.extention>\n",
    "img_names = glob.glob(PATH+'*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the model using some sample images.\n",
    "\n",
    "Testing the model with IoU (Interception over Union ratio used in non-max suppression) threshold and confidence threshold both set to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-855670b063d0>:43: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From c:\\users\\navin\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-855670b063d0>:6: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-7-91a7ea47338e>:46: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batch_size = len(img_names)\n",
    "batch = load_images(img_names, model_size=_MODEL_SIZE)\n",
    "#print(batch_size)\n",
    "class_names = load_class_names('files/coco.names')\n",
    "n_classes = len(class_names)\n",
    "max_output_size = 10\n",
    "iou_threshold = 0.5\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "model = Yolo_v3(n_classes=n_classes, model_size=_MODEL_SIZE,\n",
    "                max_output_size=max_output_size,\n",
    "                iou_threshold=iou_threshold,\n",
    "                confidence_threshold=confidence_threshold)\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [batch_size, 416, 416, 3])\n",
    "\n",
    "detections = model(inputs, training=False)\n",
    "\n",
    "model_vars = tf.global_variables(scope='yolo_v3_model')\n",
    "assign_ops = load_weights(model_vars, 'files/yolov3.weights')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(assign_ops)\n",
    "    detection_result = sess.run(detections, feed_dict={inputs: batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects_dict = dict()\n",
    "# for key, values in detection_result[0].items():\n",
    "#     if len(values):\n",
    "#         objects_dict[class_names[key]] = len(values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objects = [class_names[key] for key in detection_result[0].keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def output(words, img_names, detection_result, class_names, _MODEL_SIZE, obj):\n",
    "    \n",
    "# ### Yes No type question\n",
    "#     if words[0] == 'is' or words[0]=='are'  and 'locate' in final_words_lemmatizer:\n",
    "#         for i in obj:\n",
    "#             if i in objects_dict:\n",
    "#                 print(i,':','yes')\n",
    "#                 draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE, obj)\n",
    "#             else:\n",
    "#                 print(i,':','no')\n",
    "    \n",
    "    \n",
    "# ### Yes No type question\n",
    "#     if words[0] == 'is' or words[0]=='are':\n",
    "#         for i in obj:\n",
    "#             if i in objects_dict:\n",
    "#                 print(i,':','yes')\n",
    "#             else:\n",
    "#                 print(i,':','no')\n",
    "\n",
    "# ### To count number of objects and to locate them            \n",
    "#     elif words[0] == 'how'  and words[1]=='many' and ('locate' in final_words_lemmatizer or 'show' in words):\n",
    "#         draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE, obj)\n",
    "#         for i in obj:\n",
    "#             if i in objects_dict:\n",
    "#                 print(i,':',objects_dict[i])\n",
    "\n",
    "# ### To only count number of objects\n",
    "#     elif words[0] == 'how'  and words[1]=='many' :\n",
    "#         for i in obj:\n",
    "#             if i in objects_dict:\n",
    "#                 print(i,':',objects_dict[i])\n",
    "\n",
    "# ### To only locate the objects\n",
    "#     elif words[0] == 'where':\n",
    "#         draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE, obj)\n",
    "\n",
    "#     else:\n",
    "#         print('Invalid QUERY!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# flag = False\n",
    "# for i in obj:\n",
    "#     if i in class_names:\n",
    "#         flag = True\n",
    "#         output(words, img_names, detection_result, class_names, _MODEL_SIZE, obj)\n",
    "        \n",
    "# if flag == False:\n",
    "#     print('Sorry the object is not in the image.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image_names =[]\n",
    "for img in img_names:\n",
    "    path, name = img.split('\\\\')\n",
    "    final_image_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "object_list, repeated_detected_object_list, repeated_detected_object_bounding_boxes= draw_boxes(img_names, detection_result, class_names, _MODEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bicycle</td>\n",
       "      <td>transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>car</td>\n",
       "      <td>transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>motorbike</td>\n",
       "      <td>transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeroplane</td>\n",
       "      <td>transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>vase</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>scissors</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>teddy bear</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>hair drier</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>toothbrush</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_name category_name\n",
       "0       person        person\n",
       "1      bicycle     transport\n",
       "2          car     transport\n",
       "3    motorbike     transport\n",
       "4    aeroplane     transport\n",
       "..         ...           ...\n",
       "75        vase     household\n",
       "76    scissors     household\n",
       "77  teddy bear         other\n",
       "78  hair drier     household\n",
       "79  toothbrush     household\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = pd.read_csv('category.csv')\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['person', 'transport'],\n",
       " ['wearable', 'person', 'transport'],\n",
       " ['person', 'transport'],\n",
       " ['animal', 'furniture'],\n",
       " ['person', 'animal'],\n",
       " ['wearable', 'person', 'transport'],\n",
       " ['transport'],\n",
       " ['animal'],\n",
       " ['animal'],\n",
       " ['person', 'wearable'],\n",
       " [],\n",
       " ['transport'],\n",
       " ['other', 'person', 'gadget'],\n",
       " ['furniture', 'gadget'],\n",
       " ['gadget'],\n",
       " ['transport'],\n",
       " ['household'],\n",
       " ['person', 'household'],\n",
       " ['animal'],\n",
       " ['animal']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list = []\n",
    "\n",
    "for obj in object_list:\n",
    "    category_list.append([category[category.class_name == class_name].category_name.values[0] for class_name in obj])\n",
    "    \n",
    "category_list = [list(set(classes)) for classes in category_list]\n",
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>objects</th>\n",
       "      <th>category</th>\n",
       "      <th>repeated_objects</th>\n",
       "      <th>repeated_objects_coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bike.jpg</td>\n",
       "      <td>[motorbike, person]</td>\n",
       "      <td>[person, transport]</td>\n",
       "      <td>[person, motorbike]</td>\n",
       "      <td>[[[215.31717, 66.48491, 320.7474, 327.86932, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bike2.jpg</td>\n",
       "      <td>[car, motorbike, person, backpack]</td>\n",
       "      <td>[wearable, person, transport]</td>\n",
       "      <td>[person, person, person, person, person, perso...</td>\n",
       "      <td>[[[191.18695, 137.48477, 265.04788, 359.23596,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bike3.jpg</td>\n",
       "      <td>[car, motorbike, person, bus]</td>\n",
       "      <td>[person, transport]</td>\n",
       "      <td>[person, person, person, person, person, perso...</td>\n",
       "      <td>[[[214.96106, 84.355705, 290.92474, 223.20602,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cat.jpg</td>\n",
       "      <td>[bed, cat]</td>\n",
       "      <td>[animal, furniture]</td>\n",
       "      <td>[cat, bed]</td>\n",
       "      <td>[[[118.91197, 20.446396, 366.0003, 390.24908, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat2.jpg</td>\n",
       "      <td>[person, cat]</td>\n",
       "      <td>[person, animal]</td>\n",
       "      <td>[person, person, cat, cat]</td>\n",
       "      <td>[[[62.34414, 17.049911, 294.2313, 344.15063, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>city.jpg</td>\n",
       "      <td>[car, truck, traffic light, person, handbag, bus]</td>\n",
       "      <td>[wearable, person, transport]</td>\n",
       "      <td>[person, person, person, person, person, perso...</td>\n",
       "      <td>[[[340.6137, 281.9729, 371.4229, 410.94055, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>concept.jpg</td>\n",
       "      <td>[car]</td>\n",
       "      <td>[transport]</td>\n",
       "      <td>[car, car]</td>\n",
       "      <td>[[[348.19485, 195.86836, 411.25632, 237.2401, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dog.jpg</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[[[153.85773, 38.250046, 318.4127, 387.8401, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dog2.jpg</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[dog]</td>\n",
       "      <td>[[[138.26129, 38.907196, 347.87863, 414.3092, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>handbag.jpg</td>\n",
       "      <td>[person, handbag]</td>\n",
       "      <td>[person, wearable]</td>\n",
       "      <td>[person, person, person, person, handbag, hand...</td>\n",
       "      <td>[[[318.7868, 7.9528046, 400.81824, 409.3216, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>handbag2.jpg</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lambo.jpg</td>\n",
       "      <td>[car]</td>\n",
       "      <td>[transport]</td>\n",
       "      <td>[car]</td>\n",
       "      <td>[[[58.305923, 184.40509, 355.5642, 318.26636, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mobile.jpg</td>\n",
       "      <td>[book, person, cell phone]</td>\n",
       "      <td>[other, person, gadget]</td>\n",
       "      <td>[person, person, person, person, person, perso...</td>\n",
       "      <td>[[[238.03503, 98.4449, 376.14197, 372.24347, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phone.jpg</td>\n",
       "      <td>[cell phone, diningtable]</td>\n",
       "      <td>[furniture, gadget]</td>\n",
       "      <td>[diningtable, cell phone]</td>\n",
       "      <td>[[[6.907181, 63.512222, 409.05164, 398.18903, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phone2.jpg</td>\n",
       "      <td>[cell phone]</td>\n",
       "      <td>[gadget]</td>\n",
       "      <td>[cell phone]</td>\n",
       "      <td>[[[231.09857, 99.76085, 397.55896, 375.96375, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>swift.jpg</td>\n",
       "      <td>[car]</td>\n",
       "      <td>[transport]</td>\n",
       "      <td>[car]</td>\n",
       "      <td>[[[28.871017, 69.83183, 346.29523, 366.80957, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>toothbrush.jpg</td>\n",
       "      <td>[toothbrush]</td>\n",
       "      <td>[household]</td>\n",
       "      <td>[toothbrush, toothbrush, toothbrush, toothbrus...</td>\n",
       "      <td>[[[138.07603, 157.80118, 163.31017, 437.77267,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>toothbrush2.jpg</td>\n",
       "      <td>[person, toothbrush]</td>\n",
       "      <td>[person, household]</td>\n",
       "      <td>[person, toothbrush]</td>\n",
       "      <td>[[[256.25317, 137.4797, 411.52338, 293.23383, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>zebra.jpg</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>[[[-4.1147614, 13.063614, 332.98633, 415.4176,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>zebra2.jpg</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>[animal]</td>\n",
       "      <td>[zebra]</td>\n",
       "      <td>[[[62.734253, 45.22061, 363.69885, 370.7225, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_name                                            objects  \\\n",
       "0          bike.jpg                                [motorbike, person]   \n",
       "1         bike2.jpg                 [car, motorbike, person, backpack]   \n",
       "2         bike3.jpg                      [car, motorbike, person, bus]   \n",
       "3           cat.jpg                                         [bed, cat]   \n",
       "4          cat2.jpg                                      [person, cat]   \n",
       "5          city.jpg  [car, truck, traffic light, person, handbag, bus]   \n",
       "6       concept.jpg                                              [car]   \n",
       "7           dog.jpg                                              [dog]   \n",
       "8          dog2.jpg                                              [dog]   \n",
       "9       handbag.jpg                                  [person, handbag]   \n",
       "10     handbag2.jpg                                                 []   \n",
       "11        lambo.jpg                                              [car]   \n",
       "12       mobile.jpg                         [book, person, cell phone]   \n",
       "13        phone.jpg                          [cell phone, diningtable]   \n",
       "14       phone2.jpg                                       [cell phone]   \n",
       "15        swift.jpg                                              [car]   \n",
       "16   toothbrush.jpg                                       [toothbrush]   \n",
       "17  toothbrush2.jpg                               [person, toothbrush]   \n",
       "18        zebra.jpg                                            [zebra]   \n",
       "19       zebra2.jpg                                            [zebra]   \n",
       "\n",
       "                         category  \\\n",
       "0             [person, transport]   \n",
       "1   [wearable, person, transport]   \n",
       "2             [person, transport]   \n",
       "3             [animal, furniture]   \n",
       "4                [person, animal]   \n",
       "5   [wearable, person, transport]   \n",
       "6                     [transport]   \n",
       "7                        [animal]   \n",
       "8                        [animal]   \n",
       "9              [person, wearable]   \n",
       "10                             []   \n",
       "11                    [transport]   \n",
       "12        [other, person, gadget]   \n",
       "13            [furniture, gadget]   \n",
       "14                       [gadget]   \n",
       "15                    [transport]   \n",
       "16                    [household]   \n",
       "17            [person, household]   \n",
       "18                       [animal]   \n",
       "19                       [animal]   \n",
       "\n",
       "                                     repeated_objects  \\\n",
       "0                                 [person, motorbike]   \n",
       "1   [person, person, person, person, person, perso...   \n",
       "2   [person, person, person, person, person, perso...   \n",
       "3                                          [cat, bed]   \n",
       "4                          [person, person, cat, cat]   \n",
       "5   [person, person, person, person, person, perso...   \n",
       "6                                          [car, car]   \n",
       "7                                               [dog]   \n",
       "8                                               [dog]   \n",
       "9   [person, person, person, person, handbag, hand...   \n",
       "10                                                 []   \n",
       "11                                              [car]   \n",
       "12  [person, person, person, person, person, perso...   \n",
       "13                          [diningtable, cell phone]   \n",
       "14                                       [cell phone]   \n",
       "15                                              [car]   \n",
       "16  [toothbrush, toothbrush, toothbrush, toothbrus...   \n",
       "17                               [person, toothbrush]   \n",
       "18                                            [zebra]   \n",
       "19                                            [zebra]   \n",
       "\n",
       "                         repeated_objects_coordinates  \n",
       "0   [[[215.31717, 66.48491, 320.7474, 327.86932, 0...  \n",
       "1   [[[191.18695, 137.48477, 265.04788, 359.23596,...  \n",
       "2   [[[214.96106, 84.355705, 290.92474, 223.20602,...  \n",
       "3   [[[118.91197, 20.446396, 366.0003, 390.24908, ...  \n",
       "4   [[[62.34414, 17.049911, 294.2313, 344.15063, 0...  \n",
       "5   [[[340.6137, 281.9729, 371.4229, 410.94055, 0....  \n",
       "6   [[[348.19485, 195.86836, 411.25632, 237.2401, ...  \n",
       "7   [[[153.85773, 38.250046, 318.4127, 387.8401, 0...  \n",
       "8   [[[138.26129, 38.907196, 347.87863, 414.3092, ...  \n",
       "9   [[[318.7868, 7.9528046, 400.81824, 409.3216, 0...  \n",
       "10                                                 []  \n",
       "11  [[[58.305923, 184.40509, 355.5642, 318.26636, ...  \n",
       "12  [[[238.03503, 98.4449, 376.14197, 372.24347, 0...  \n",
       "13  [[[6.907181, 63.512222, 409.05164, 398.18903, ...  \n",
       "14  [[[231.09857, 99.76085, 397.55896, 375.96375, ...  \n",
       "15  [[[28.871017, 69.83183, 346.29523, 366.80957, ...  \n",
       "16  [[[138.07603, 157.80118, 163.31017, 437.77267,...  \n",
       "17  [[[256.25317, 137.4797, 411.52338, 293.23383, ...  \n",
       "18  [[[-4.1147614, 13.063614, 332.98633, 415.4176,...  \n",
       "19  [[[62.734253, 45.22061, 363.69885, 370.7225, 0...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pd.DataFrame(columns=['image_name', 'objects','category','repeated_objects','repeated_objects_coordinates'])\n",
    "file['objects'] = file['objects'].astype('object')\n",
    "file['category'] = file['category'].astype('object')\n",
    "file['image_name'] = final_image_names\n",
    "file['objects'] = object_list\n",
    "file['category'] = category_list\n",
    "file['repeated_objects'] = repeated_detected_object_list\n",
    "file['repeated_objects_coordinates'] = repeated_detected_object_bounding_boxes\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    }
   ],
   "source": [
    "file.to_csv(\"tag_data.csv\", index=False)\n",
    "print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_image = pd.DataFrame({'category':file.category.sum(), 'image':file.image_name.repeat(file.category.str.len())})\n",
    "categorical_image.to_csv(\"category_wise_image.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person</td>\n",
       "      <td>bike.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transport</td>\n",
       "      <td>bike.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>person</td>\n",
       "      <td>bike2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wearable</td>\n",
       "      <td>bike2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transport</td>\n",
       "      <td>bike2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person</td>\n",
       "      <td>bike3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transport</td>\n",
       "      <td>bike3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furniture</td>\n",
       "      <td>cat.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>animal</td>\n",
       "      <td>cat.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person</td>\n",
       "      <td>cat2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>animal</td>\n",
       "      <td>cat2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>city.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wearable</td>\n",
       "      <td>city.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transport</td>\n",
       "      <td>city.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>transport</td>\n",
       "      <td>concept.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>animal</td>\n",
       "      <td>dog.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>animal</td>\n",
       "      <td>dog2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>person</td>\n",
       "      <td>handbag.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wearable</td>\n",
       "      <td>handbag.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>transport</td>\n",
       "      <td>lambo.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>person</td>\n",
       "      <td>mobile.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gadget</td>\n",
       "      <td>mobile.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>other</td>\n",
       "      <td>mobile.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>furniture</td>\n",
       "      <td>phone.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gadget</td>\n",
       "      <td>phone.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gadget</td>\n",
       "      <td>phone2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>transport</td>\n",
       "      <td>swift.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>household</td>\n",
       "      <td>toothbrush.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>person</td>\n",
       "      <td>toothbrush2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>household</td>\n",
       "      <td>toothbrush2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>animal</td>\n",
       "      <td>zebra.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>animal</td>\n",
       "      <td>zebra2.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category            image\n",
       "0      person         bike.jpg\n",
       "0   transport         bike.jpg\n",
       "1      person        bike2.jpg\n",
       "1    wearable        bike2.jpg\n",
       "1   transport        bike2.jpg\n",
       "2      person        bike3.jpg\n",
       "2   transport        bike3.jpg\n",
       "3   furniture          cat.jpg\n",
       "3      animal          cat.jpg\n",
       "4      person         cat2.jpg\n",
       "4      animal         cat2.jpg\n",
       "5      person         city.jpg\n",
       "5    wearable         city.jpg\n",
       "5   transport         city.jpg\n",
       "6   transport      concept.jpg\n",
       "7      animal          dog.jpg\n",
       "8      animal         dog2.jpg\n",
       "9      person      handbag.jpg\n",
       "9    wearable      handbag.jpg\n",
       "11  transport        lambo.jpg\n",
       "12     person       mobile.jpg\n",
       "12     gadget       mobile.jpg\n",
       "12      other       mobile.jpg\n",
       "13  furniture        phone.jpg\n",
       "13     gadget        phone.jpg\n",
       "14     gadget       phone2.jpg\n",
       "15  transport        swift.jpg\n",
       "16  household   toothbrush.jpg\n",
       "17     person  toothbrush2.jpg\n",
       "17  household  toothbrush2.jpg\n",
       "18     animal        zebra.jpg\n",
       "19     animal       zebra2.jpg"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000224893E3808>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['category', 'image']\n",
    "cat_group = categorical_image.groupby(a)\n",
    "cat_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
